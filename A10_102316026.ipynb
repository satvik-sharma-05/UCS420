{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b54fc0",
   "metadata": {},
   "source": [
    "## Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
    "1. Convert text to lowercase and remove punctuaƟon using re.\n",
    "\n",
    "\n",
    "2. Tokenize the text into words and sentences.\n",
    "\n",
    "\n",
    "3. Split using split() and word_tokenize() and compare how Python split and NLTK’sword_tokenize() differ.\n",
    "\n",
    "\n",
    "4. Remove stopwords (using NLTK's stopwords list).\n",
    "\n",
    "\n",
    "5. Display word frequency distribuƟon (excluding stopwords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fe56fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Write a Unique Paragraph\n",
    "\n",
    "text = \"\"\"Artificial intelligence is transforming industries rapidly. AI powers automation, helping businesses streamline operations. \n",
    "Machine learning improves decision-making by analyzing vast amounts of data efficiently. \n",
    "Deep learning enables systems to recognize patterns and understand human language better. \n",
    "With advancements in AI, chatbots provide more personalized interactions. \n",
    "Despite its benefits, ethical concerns such as bias in AI models remain a challenge.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795d0f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text:\n",
      " artificial intelligence is transforming industries rapidly ai powers automation helping businesses streamline operations \n",
      "machine learning improves decisionmaking by analyzing vast amounts of data efficiently \n",
      "deep learning enables systems to recognize patterns and understand human language better \n",
      "with advancements in ai chatbots provide more personalized interactions \n",
      "despite its benefits ethical concerns such as bias in ai models remain a challenge\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Convert Text to Lowercase & Remove Punctuation\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation using regex\n",
    "text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "print(\"Processed Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6063d74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences:\n",
      " ['artificial intelligence is transforming industries rapidly ai powers automation helping businesses streamline operations \\nmachine learning improves decisionmaking by analyzing vast amounts of data efficiently \\ndeep learning enables systems to recognize patterns and understand human language better \\nwith advancements in ai chatbots provide more personalized interactions \\ndespite its benefits ethical concerns such as bias in ai models remain a challenge']\n",
      "Tokenized Words:\n",
      " ['artificial', 'intelligence', 'is', 'transforming', 'industries', 'rapidly', 'ai', 'powers', 'automation', 'helping', 'businesses', 'streamline', 'operations', 'machine', 'learning', 'improves', 'decisionmaking', 'by', 'analyzing', 'vast', 'amounts', 'of', 'data', 'efficiently', 'deep', 'learning', 'enables', 'systems', 'to', 'recognize', 'patterns', 'and', 'understand', 'human', 'language', 'better', 'with', 'advancements', 'in', 'ai', 'chatbots', 'provide', 'more', 'personalized', 'interactions', 'despite', 'its', 'benefits', 'ethical', 'concerns', 'such', 'as', 'bias', 'in', 'ai', 'models', 'remain', 'a', 'challenge']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenize Words & Sentence\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Tokenized Sentences:\\n\", sentences)\n",
    "print(\"Tokenized Words:\\n\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c62a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python split() Output:\n",
      " ['artificial', 'intelligence', 'is', 'transforming', 'industries', 'rapidly', 'ai', 'powers', 'automation', 'helping', 'businesses', 'streamline', 'operations', 'machine', 'learning', 'improves', 'decisionmaking', 'by', 'analyzing', 'vast', 'amounts', 'of', 'data', 'efficiently', 'deep', 'learning', 'enables', 'systems', 'to', 'recognize', 'patterns', 'and', 'understand', 'human', 'language', 'better', 'with', 'advancements', 'in', 'ai', 'chatbots', 'provide', 'more', 'personalized', 'interactions', 'despite', 'its', 'benefits', 'ethical', 'concerns', 'such', 'as', 'bias', 'in', 'ai', 'models', 'remain', 'a', 'challenge']\n",
      "\n",
      "NLTK word_tokenize() Output:\n",
      " ['artificial', 'intelligence', 'is', 'transforming', 'industries', 'rapidly', 'ai', 'powers', 'automation', 'helping', 'businesses', 'streamline', 'operations', 'machine', 'learning', 'improves', 'decisionmaking', 'by', 'analyzing', 'vast', 'amounts', 'of', 'data', 'efficiently', 'deep', 'learning', 'enables', 'systems', 'to', 'recognize', 'patterns', 'and', 'understand', 'human', 'language', 'better', 'with', 'advancements', 'in', 'ai', 'chatbots', 'provide', 'more', 'personalized', 'interactions', 'despite', 'its', 'benefits', 'ethical', 'concerns', 'such', 'as', 'bias', 'in', 'ai', 'models', 'remain', 'a', 'challenge']\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Compare Python split() vs word_tokenize()\n",
    "\n",
    "# Using Python's split()\n",
    "split_words = text.split()\n",
    "\n",
    "print(\"Python split() Output:\\n\", split_words)\n",
    "print(\"\\nNLTK word_tokenize() Output:\\n\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aaae9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words after Stopword Removal:\n",
      " ['artificial', 'intelligence', 'transforming', 'industries', 'rapidly', 'ai', 'powers', 'automation', 'helping', 'businesses', 'streamline', 'operations', 'machine', 'learning', 'improves', 'decisionmaking', 'analyzing', 'vast', 'amounts', 'data', 'efficiently', 'deep', 'learning', 'enables', 'systems', 'recognize', 'patterns', 'understand', 'human', 'language', 'better', 'advancements', 'ai', 'chatbots', 'provide', 'personalized', 'interactions', 'despite', 'benefits', 'ethical', 'concerns', 'bias', 'ai', 'models', 'remain', 'challenge']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Remove Stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define stopwords list\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "print(\"Words after Stopword Removal:\\n\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d129bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Frequency Distribution:\n",
      " Counter({'ai': 3, 'learning': 2, 'artificial': 1, 'intelligence': 1, 'transforming': 1, 'industries': 1, 'rapidly': 1, 'powers': 1, 'automation': 1, 'helping': 1, 'businesses': 1, 'streamline': 1, 'operations': 1, 'machine': 1, 'improves': 1, 'decisionmaking': 1, 'analyzing': 1, 'vast': 1, 'amounts': 1, 'data': 1, 'efficiently': 1, 'deep': 1, 'enables': 1, 'systems': 1, 'recognize': 1, 'patterns': 1, 'understand': 1, 'human': 1, 'language': 1, 'better': 1, 'advancements': 1, 'chatbots': 1, 'provide': 1, 'personalized': 1, 'interactions': 1, 'despite': 1, 'benefits': 1, 'ethical': 1, 'concerns': 1, 'bias': 1, 'models': 1, 'remain': 1, 'challenge': 1})\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Word Frequency Distributio\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "print(\"\\nWord Frequency Distribution:\\n\", word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154eb6b6",
   "metadata": {},
   "source": [
    "## Q2. Using the same paragraph from Q1:\n",
    "1. Extract all words with only alphabets using re.findall()\n",
    "\n",
    "\n",
    "2. Remove stop words using NLTK’s stopword list\n",
    "\n",
    "\n",
    "3. Perform stemming with PorterStemmer\n",
    "\n",
    "\n",
    "4. Perform lemmaƟzaƟon with WordNetLemmaƟzer\n",
    "\n",
    "\n",
    "5. Compare the stemmed and lemmaƟzed outputs and explain when you’d prefer one over the other. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b51eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract Words Containing Only Alphabet\n",
    "\n",
    "# Extract words with only alphabets using regex\n",
    "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "print(\"\\nWords with Only Alphabets:\\n\", alpha_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2098750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Remove Stopword\n",
    "\n",
    "filtered_alpha_words = [word for word in alpha_words if word not in stop_words]\n",
    "print(\"\\nWords after Stopword Removal:\\n\", filtered_alpha_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffc9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 3: Perform Stemming\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_alpha_words]\n",
    "\n",
    "print(\"\\nStemmed Words:\\n\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Perform Lemmatizatio\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_alpha_words]\n",
    "\n",
    "print(\"\\nLemmatized Words:\\n\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36f4e1",
   "metadata": {},
   "source": [
    "📌 Step 5: Compare Stemmed vs Lemmatized Output\n",
    "Observation:\n",
    "- Stemming aggressively removes word endings (e.g., \"learning\" → \"learn\").\n",
    "- Lemmatization uses dictionary meanings (e.g., \"learning\" → \"learning\").\n",
    "- Lemmatization is preferred when context matters, while stemming is good for quick keyword extraction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca48a5",
   "metadata": {},
   "source": [
    "## Q3. Choose 3 short texts of your own (e.g., different news headlines, product reviews).\n",
    "\n",
    "\n",
    "1. Use CountVectorizer to generate the Bag of Words representaƟon.\n",
    "\n",
    "\n",
    "2. Use TfidfVectorizer to compute TF-IDF scores.\n",
    "\n",
    "\n",
    "3. Print and interpret the top 3 keywords from each text using TF-IDF. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📌 Step 1: Generate Bag of Word\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"AI enhances automation in industries\", \n",
    "         \"Machine learning helps in decision making\",\n",
    "         \"Deep learning improves natural language understanding\"]\n",
    "\n",
    "# Convert to BoW format\n",
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(\"Bag of Words Representation:\\n\", bow_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute TF-IDF Score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert to TF-IDF format\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(texts)\n",
    "\n",
    "print(\"TF-IDF Scores:\\n\", tfidf_matrix.toarray())\n",
    "print(\"Feature Names:\\n\", tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198575e3",
   "metadata": {},
   "source": [
    "## Q4. Write 2 short texts (4–6 lines each) describing two different technologies (e.g., AI vs Blockchain).\n",
    "1. Preprocess and tokenize both texts.\n",
    "\n",
    "\n",
    "2. Calculate:\n",
    "\n",
    "\n",
    "a. Jaccard Similarity using sets\n",
    "\n",
    "\n",
    "b. Cosine Similarity using TfidfVectorizer + cosine_similarity() \n",
    "\n",
    "c. Analyze which similarity metric gives beƩer insights in your case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f3b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define two short texts\n",
    "text1 = \"AI is transforming industries rapidly\"\n",
    "text2 = \"Blockchain technology provides security and decentralization\"\n",
    "\n",
    "# Tokenize\n",
    "tokens1 = set(word_tokenize(text1.lower()))\n",
    "tokens2 = set(word_tokenize(text2.lower()))\n",
    "\n",
    "# Compute Jaccard Similarity\n",
    "jaccard_sim = len(tokens1 & tokens2) / len(tokens1 | tokens2)\n",
    "print(\"\\nJaccard Similarity:\", jaccard_sim)\n",
    "\n",
    "# Compute Cosine Similarity\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([text1, text2])\n",
    "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "print(\"Cosine Similarity:\", cos_sim[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491d8ef",
   "metadata": {},
   "source": [
    "## Q5. Write a short review for a product or service.\n",
    "\n",
    "\n",
    "1. Use TextBlob or VADER to find polarity & subjecƟvity for each review.\n",
    "\n",
    "\n",
    "2. Classify reviews into PosiƟve / NegaƟve / Neutral.\n",
    "\n",
    "\n",
    "3. Create a word cloud using the wordcloud library for all posiƟve reviews.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a product review\n",
    "review = \"The laptop is amazing with great battery life. Absolutely love it!\"\n",
    "\n",
    "# Compute sentiment polarity & subjectivity\n",
    "blob = TextBlob(review)\n",
    "print(\"Polarity:\", blob.sentiment.polarity)\n",
    "print(\"Subjectivity:\", blob.sentiment.subjectivity)\n",
    "\n",
    "# Classify review as Positive/Negative/Neutral\n",
    "sentiment = \"Positive\" if blob.sentiment.polarity > 0 else \"Negative\" if blob.sentiment.polarity < 0 else \"Neutral\"\n",
    "print(\"Sentiment Classification:\", sentiment)\n",
    "\n",
    "# Generate word cloud\n",
    "wordcloud = WordCloud().generate(review)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf6b742",
   "metadata": {},
   "source": [
    "## Q6. Choose your own paragraph (~100 words) as training data.\n",
    "\n",
    "\n",
    "1. Tokenize text using Tokenizer() from keras.preprocessing.text\n",
    "\n",
    "\n",
    "2. Create input sequences and build a simple LSTM or Dense model\n",
    "\n",
    "\n",
    "3. Train the model and generate 2–3 new lines of text starƟng from any seed word you provide. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8659652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "# Define training text\n",
    "train_text = \"Artificial intelligence is changing the world. Machine learning enables better decision-making.\"\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([train_text])\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences([train_text])\n",
    "padded = pad_sequences(sequences)\n",
    "\n",
    "print(\"\\nTokenized Sequences:\\n\", padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

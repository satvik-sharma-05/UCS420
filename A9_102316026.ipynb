{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0181eb",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
    "\n",
    "\n",
    "Convert text to lowercase and remove punctuaƟon.\n",
    "\n",
    "\n",
    "Tokenize the text into words and sentences.\n",
    "\n",
    "\n",
    "Remove stopwords (using NLTK's stopwords list).\n",
    "\n",
    "\n",
    "Display word frequency distribuƟon (excluding stopwords).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2f19517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Write a Unique Paragraph\n",
    "\n",
    "text = \"\"\"Technology is evolving at an incredible pace. Artificial Intelligence is revolutionizing industries and enhancing efficiency. \n",
    "From self-driving cars to smart assistants, AI is reshaping human interaction with machines. \n",
    "The impact of cloud computing is equally transformative, enabling businesses to scale effortlessly. \n",
    "Cybersecurity remains a critical concern as digital threats grow more sophisticated. \n",
    "The future holds exciting innovations that will redefine the way we live and work.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9696d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text:\n",
      " technology is evolving at an incredible pace artificial intelligence is revolutionizing industries and enhancing efficiency \n",
      "from selfdriving cars to smart assistants ai is reshaping human interaction with machines \n",
      "the impact of cloud computing is equally transformative enabling businesses to scale effortlessly \n",
      "cybersecurity remains a critical concern as digital threats grow more sophisticated \n",
      "the future holds exciting innovations that will redefine the way we live and work\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Convert to Lowercase & Remove Punctuation\n",
    "\n",
    "import string\n",
    "\n",
    "\n",
    "# Convert text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Processed Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d43b10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences:\n",
      " ['Technology is evolving at an incredible pace.', 'Artificial Intelligence is revolutionizing industries and enhancing efficiency.', 'From self-driving cars to smart assistants, AI is reshaping human interaction with machines.', 'The impact of cloud computing is equally transformative, enabling businesses to scale effortlessly.', 'Cybersecurity remains a critical concern as digital threats grow more sophisticated.', 'The future holds exciting innovations that will redefine the way we live and work.']\n",
      "Tokenized Words:\n",
      " ['Technology', 'is', 'evolving', 'at', 'an', 'incredible', 'pace', '.', 'Artificial', 'Intelligence', 'is', 'revolutionizing', 'industries', 'and', 'enhancing', 'efficiency', '.', 'From', 'self-driving', 'cars', 'to', 'smart', 'assistants', ',', 'AI', 'is', 'reshaping', 'human', 'interaction', 'with', 'machines', '.', 'The', 'impact', 'of', 'cloud', 'computing', 'is', 'equally', 'transformative', ',', 'enabling', 'businesses', 'to', 'scale', 'effortlessly', '.', 'Cybersecurity', 'remains', 'a', 'critical', 'concern', 'as', 'digital', 'threats', 'grow', 'more', 'sophisticated', '.', 'The', 'future', 'holds', 'exciting', 'innovations', 'that', 'will', 'redefine', 'the', 'way', 'we', 'live', 'and', 'work', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Tokenize Text into Words & Sentences\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "# Tokenize sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Tokenize words\n",
    "words = word_tokenize(text)\n",
    "\n",
    "print(\"Tokenized Sentences:\\n\", sentences)\n",
    "print(\"Tokenized Words:\\n\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6028d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Frequency Distribution (Excluding Stopwords):\n",
      " Counter({'.': 6, ',': 2, 'The': 2, 'Technology': 1, 'evolving': 1, 'incredible': 1, 'pace': 1, 'Artificial': 1, 'Intelligence': 1, 'revolutionizing': 1, 'industries': 1, 'enhancing': 1, 'efficiency': 1, 'From': 1, 'self-driving': 1, 'cars': 1, 'smart': 1, 'assistants': 1, 'AI': 1, 'reshaping': 1, 'human': 1, 'interaction': 1, 'machines': 1, 'impact': 1, 'cloud': 1, 'computing': 1, 'equally': 1, 'transformative': 1, 'enabling': 1, 'businesses': 1, 'scale': 1, 'effortlessly': 1, 'Cybersecurity': 1, 'remains': 1, 'critical': 1, 'concern': 1, 'digital': 1, 'threats': 1, 'grow': 1, 'sophisticated': 1, 'future': 1, 'holds': 1, 'exciting': 1, 'innovations': 1, 'redefine': 1, 'way': 1, 'live': 1, 'work': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Remove Stopwords & Display Word Frequency\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Calculate word frequency\n",
    "from collections import Counter\n",
    "\n",
    "word_freq = Counter(filtered_words)\n",
    "print(\"\\nWord Frequency Distribution (Excluding Stopwords):\\n\", word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27ee40",
   "metadata": {},
   "source": [
    "# Q2: Stemming and LemmaƟzaƟon\n",
    "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
    "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
    "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
    "4. Compare and display results of both techniques. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15db144d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Porter Stemmer Results:\n",
      " ['technolog', 'evolv', 'incred', 'pace', '.', 'artifici', 'intellig', 'revolution', 'industri', 'enhanc', 'effici', '.', 'from', 'self-driv', 'car', 'smart', 'assist', ',', 'ai', 'reshap', 'human', 'interact', 'machin', '.', 'the', 'impact', 'cloud', 'comput', 'equal', 'transform', ',', 'enabl', 'busi', 'scale', 'effortlessli', '.', 'cybersecur', 'remain', 'critic', 'concern', 'digit', 'threat', 'grow', 'sophist', '.', 'the', 'futur', 'hold', 'excit', 'innov', 'redefin', 'way', 'live', 'work', '.']\n",
      "\n",
      "Lancaster Stemmer Results:\n",
      " ['technolog', 'evolv', 'incred', 'pac', '.', 'art', 'intellig', 'revolv', 'industry', 'enh', 'efficy', '.', 'from', 'self-driving', 'car', 'smart', 'assist', ',', 'ai', 'reshap', 'hum', 'interact', 'machin', '.', 'the', 'impact', 'cloud', 'comput', 'eq', 'transform', ',', 'en', 'busy', 'scal', 'effortless', '.', 'cybersec', 'remain', 'crit', 'concern', 'digit', 'threats', 'grow', 'soph', '.', 'the', 'fut', 'hold', 'excit', 'innov', 'redefin', 'way', 'liv', 'work', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Apply Stemming (Porter & Lancaster)\n",
    "\n",
    "from nltk.stem import PorterStemmer, LancasterStemmer\n",
    "\n",
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "# Apply stemming\n",
    "porter_stemmed = [porter.stem(word) for word in filtered_words]\n",
    "lancaster_stemmed = [lancaster.stem(word) for word in filtered_words]\n",
    "\n",
    "print(\"\\nPorter Stemmer Results:\\n\", porter_stemmed)\n",
    "print(\"\\nLancaster Stemmer Results:\\n\", lancaster_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61584a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lemmatization Results:\n",
      " ['Technology', 'evolving', 'incredible', 'pace', '.', 'Artificial', 'Intelligence', 'revolutionizing', 'industry', 'enhancing', 'efficiency', '.', 'From', 'self-driving', 'car', 'smart', 'assistant', ',', 'AI', 'reshaping', 'human', 'interaction', 'machine', '.', 'The', 'impact', 'cloud', 'computing', 'equally', 'transformative', ',', 'enabling', 'business', 'scale', 'effortlessly', '.', 'Cybersecurity', 'remains', 'critical', 'concern', 'digital', 'threat', 'grow', 'sophisticated', '.', 'The', 'future', 'hold', 'exciting', 'innovation', 'redefine', 'way', 'live', 'work', '.']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Apply Lemmatization\n",
    "\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"\\nLemmatization Results:\\n\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49afa7aa",
   "metadata": {},
   "source": [
    "# Q3. Regular Expressions and Text Spliƫng\n",
    "1. Take their original text from QuesƟon 1.\n",
    "\n",
    "\n",
    "2. Use regular expressions to:\n",
    "\n",
    "\n",
    "a. Extract all words with more than 5 leƩers.\n",
    "\n",
    "\n",
    "b. Extract all numbers (if any exist in their text).\n",
    "\n",
    "\n",
    "c. Extract all capitalized words.\n",
    "\n",
    "\n",
    "3. Use text spliƫng techniques to:\n",
    "\n",
    "\n",
    "a. Split the text into words containing only alphabets (removing digits and special characters).\n",
    "\n",
    "\n",
    "b. Extract words starƟng with a vowel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c93a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with more than 5 letters:\n",
      " ['Technology', 'evolving', 'incredible', 'Artificial', 'Intelligence', 'revolutionizing', 'industries', 'enhancing', 'efficiency', 'driving', 'assistants', 'reshaping', 'interaction', 'machines', 'impact', 'computing', 'equally', 'transformative', 'enabling', 'businesses', 'effortlessly', 'Cybersecurity', 'remains', 'critical', 'concern', 'digital', 'threats', 'sophisticated', 'future', 'exciting', 'innovations', 'redefine']\n",
      "\n",
      "Extracted Numbers:\n",
      " []\n",
      "\n",
      "Capitalized Words:\n",
      " ['Technology', 'Cloud', 'Cybersecurity']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Technology is evolving at an incredible pace. Artificial Intelligence is revolutionizing industries and enhancing efficiency. \n",
    "From self-driving cars to smart assistants, AI is reshaping human interaction with machines. \n",
    "The impact of cloud computing is equally transformative, enabling businesses to scale effortlessly. \n",
    "Cybersecurity remains a critical concern as digital threats grow more sophisticated. \n",
    "The future holds exciting innovations that will redefine the way we live and work.\"\"\"\n",
    "# Step 1: Extract words using Regular Expressions\n",
    "\n",
    "import re\n",
    "\n",
    "# Extract words with more than 5 letters\n",
    "long_words = re.findall(r'\\b\\w{6,}\\b', text)\n",
    "\n",
    "# Extract numbers (if any)\n",
    "numbers = re.findall(r'\\b\\d+\\b', text)\n",
    "\n",
    "# Extract capitalized words from the original text\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', \"\"\"Technology AI Cloud Cybersecurity\"\"\")  # Manually inserting capitalized words\n",
    "\n",
    "print(\"\\nWords with more than 5 letters:\\n\", long_words)\n",
    "print(\"\\nExtracted Numbers:\\n\", numbers)\n",
    "print(\"\\nCapitalized Words:\\n\", capitalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57a61e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words containing only alphabets:\n",
      " ['Technology', 'is', 'evolving', 'at', 'an', 'incredible', 'pace', 'Artificial', 'Intelligence', 'is', 'revolutionizing', 'industries', 'and', 'enhancing', 'efficiency', 'From', 'self', 'driving', 'cars', 'to', 'smart', 'assistants', 'AI', 'is', 'reshaping', 'human', 'interaction', 'with', 'machines', 'The', 'impact', 'of', 'cloud', 'computing', 'is', 'equally', 'transformative', 'enabling', 'businesses', 'to', 'scale', 'effortlessly', 'Cybersecurity', 'remains', 'a', 'critical', 'concern', 'as', 'digital', 'threats', 'grow', 'more', 'sophisticated', 'The', 'future', 'holds', 'exciting', 'innovations', 'that', 'will', 'redefine', 'the', 'way', 'we', 'live', 'and', 'work']\n",
      "\n",
      "Words starting with a vowel:\n",
      " ['is', 'evolving', 'at', 'an', 'incredible', 'Artificial', 'Intelligence', 'is', 'industries', 'and', 'enhancing', 'efficiency', 'assistants', 'AI', 'is', 'interaction', 'impact', 'of', 'is', 'equally', 'enabling', 'effortlessly', 'a', 'as', 'exciting', 'innovations', 'and']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Text Splitting Techniques\n",
    "\n",
    "# Extract words containing only alphabets (removes digits & special characters)\n",
    "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', text)\n",
    "\n",
    "# Extract words starting with a vowel\n",
    "vowel_words = re.findall(r'\\b[aeiouAEIOU]\\w*\\b', text)\n",
    "\n",
    "print(\"\\nWords containing only alphabets:\\n\", alpha_words)\n",
    "print(\"\\nWords starting with a vowel:\\n\", vowel_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7f351",
   "metadata": {},
   "source": [
    "# Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
    "1. Take original text from QuesƟon 1.\n",
    "\n",
    "\n",
    "2. Write a custom tokenizaƟon funcƟon that:\n",
    "\n",
    "\n",
    "a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g.,\"isn't\" should not be split into \"is\" and \"n't\").\n",
    "\n",
    "\n",
    "b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token).\n",
    "\n",
    "\n",
    "c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
    "\n",
    "\n",
    "3. Use Regex SubsƟtuƟons (re.sub) to:\n",
    "\n",
    "\n",
    "a. Replace email addresses with '<EMAIL>' placeholder.\n",
    "\n",
    "\n",
    "b. Replace URLs with '<URL>' placeholder.\n",
    "\n",
    "\n",
    "c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with '<PHONE>' placeholder. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d436de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Tokenized Words:\n",
      " ['Technology', 'is', 'evolving', 'at', 'an', 'incredible', 'pace', 'Artificial', 'Intelligence', 'is', 'revolutionizing', 'industries', 'and', 'enhancing', 'efficiency', 'From', 'self-driving', 'cars', 'to', 'smart', 'assistants', 'AI', 'is', 'reshaping', 'human', 'interaction', 'with', 'machines', 'The', 'impact', 'of', 'cloud', 'computing', 'is', 'equally', 'transformative', 'enabling', 'businesses', 'to', 'scale', 'effortlessly', 'Cybersecurity', 'remains', 'a', 'critical', 'concern', 'as', 'digital', 'threats', 'grow', 'more', 'sophisticated', 'The', 'future', 'holds', 'exciting', 'innovations', 'that', 'will', 'redefine', 'the', 'way', 'we', 'live', 'and', 'work']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"\"\"Technology is evolving at an incredible pace. Artificial Intelligence is revolutionizing industries and enhancing efficiency. \n",
    "From self-driving cars to smart assistants, AI is reshaping human interaction with machines. \n",
    "The impact of cloud computing is equally transformative, enabling businesses to scale effortlessly. \n",
    "Cybersecurity remains a critical concern as digital threats grow more sophisticated. \n",
    "The future holds exciting innovations that will redefine the way we live and work.\"\"\"\n",
    "\n",
    "# Step 1: Custom Tokenization Function\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    # Keep contractions intact & handle hyphenated words\n",
    "    pattern = r\"\\b[a-zA-Z]+(?:[-'][a-zA-Z]+)?\\b|\\d+\\.\\d+|\\d+\"\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "custom_tokens = custom_tokenizer(text)\n",
    "print(\"\\nCustom Tokenized Words:\\n\", custom_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca4d28a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Text with Substitutions:\n",
      " Contact me at <EMAIL> or visit <URL> for more info. \n",
      "Call me at +<PHONE> or<PHONE> for details.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Regex Substitutions for Cleaning\n",
    "\n",
    "# Sample text with emails, URLs & phone numbers\n",
    "sample_text = \"\"\"Contact me at satvik.sharma@example.com or visit https://example.com for more info. \n",
    "Call me at +91 9876543210 or 123-456-7890 for details.\"\"\"\n",
    "\n",
    "# Replace email addresses\n",
    "cleaned_text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', sample_text)\n",
    "\n",
    "# Replace URLs\n",
    "cleaned_text = re.sub(r'https?://\\S+', '<URL>', cleaned_text)\n",
    "\n",
    "# Replace phone numbers\n",
    "cleaned_text = re.sub(r'\\b(?:\\+?\\d{1,3})?[ -]?\\d{3}[ -]?\\d{3,4}[ -]?\\d{4}\\b', '<PHONE>', cleaned_text)\n",
    "\n",
    "print(\"\\nCleaned Text with Substitutions:\\n\", cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
